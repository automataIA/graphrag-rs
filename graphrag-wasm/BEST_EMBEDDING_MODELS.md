# Top 3 ONNX Embedding Models for GraphRAG

Modelli selezionati basati su ricerca web 2024-2025 per GraphRAG, semantic search e RAG applications.

## ü•á #1: all-MiniLM-L6-v2 (RECOMMENDED FOR GRAPHRAG-WASM)

**Perch√© √® il migliore per noi:**
- ‚úÖ Lightweight (90.4 MB)
- ‚úÖ Ottimizzato per browser/WebAssembly
- ‚úÖ Eccellente balance velocit√†/qualit√†
- ‚úÖ Ampio supporto community
- ‚úÖ Gi√† usato nel nostro tokenizer.json

**Specifiche:**
- **Dimensioni embedding**: 384
- **Dimensioni modello**: ~90 MB
- **Velocit√†**: Molto veloce (ideale per browser)
- **Lingue**: Principalmente inglese
- **Contesto**: Fino a 128 tokens

**URL Download:**

1. **Xenova/all-MiniLM-L6-v2** (CONSIGLIATO - ottimizzato per web)
   - Repository: https://huggingface.co/Xenova/all-MiniLM-L6-v2
   - ONNX file: https://huggingface.co/Xenova/all-MiniLM-L6-v2/resolve/main/onnx/model.onnx
   - Quantized: https://huggingface.co/Xenova/all-MiniLM-L6-v2/resolve/main/onnx/model_quantized.onnx

2. **sentence-transformers/all-MiniLM-L6-v2** (originale)
   - ONNX file: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/onnx/model.onnx

3. **LightEmbed/sbert-all-MiniLM-L6-v2-onnx** (ottimizzato speed)
   - Repository: https://huggingface.co/LightEmbed/sbert-all-MiniLM-L6-v2-onnx

**Caso d'uso ideale:**
- Applicazioni browser/WASM
- Semantic search su documenti brevi-medi
- Knowledge graphs con velocit√† prioritaria
- Risorse limitate (mobile, edge devices)

---

## ü•à #2: bge-small-en-v1.5

**Perch√© √® secondo:**
- ‚úÖ Qualit√† superiore a MiniLM
- ‚úÖ Migliore per context-rich queries
- ‚ö†Ô∏è Pi√π pesante (~133 MB)
- ‚úÖ State-of-the-art per dimensioni piccole

**Specifiche:**
- **Dimensioni embedding**: 384
- **Dimensioni modello**: ~133 MB
- **Velocit√†**: Veloce
- **Lingue**: Principalmente inglese
- **Contesto**: Fino a 512 tokens

**URL Download:**

1. **Xenova/bge-small-en-v1.5** (CONSIGLIATO per web)
   - Repository: https://huggingface.co/Xenova/bge-small-en-v1.5
   - ONNX file: https://huggingface.co/Xenova/bge-small-en-v1.5/resolve/main/onnx/model.onnx

2. **Qdrant/bge-small-en-v1.5-onnx-Q** (quantized)
   - Repository: https://huggingface.co/Qdrant/bge-small-en-v1.5-onnx-Q

3. **BAAI/bge-small-en-v1.5** (originale)
   - Repository: https://huggingface.co/BAAI/bge-small-en-v1.5

**Caso d'uso ideale:**
- RAG con documenti complessi
- Semantic search avanzata
- Knowledge graphs enterprise
- Trade-off qualit√†/velocit√† bilanciato

---

## ü•â #3: nomic-embed-text-v1.5

**Perch√© √® terzo:**
- ‚úÖ Massima qualit√† embedding
- ‚úÖ Multilingual (100+ lingue)
- ‚úÖ Long context (8192 tokens)
- ‚ö†Ô∏è Pi√π pesante (~270 MB)
- ‚ö†Ô∏è Pi√π lento per browser

**Specifiche:**
- **Dimensioni embedding**: 768
- **Dimensioni modello**: ~270 MB
- **Velocit√†**: Moderata
- **Lingue**: 100+ lingue (multilingual)
- **Contesto**: Fino a 8192 tokens

**URL Download:**

1. **nomic-ai/nomic-embed-text-v1.5** (ufficiale)
   - Repository: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5
   - ONNX file: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5/resolve/main/onnx/model.onnx

**Caso d'uso ideale:**
- Documenti lunghi (papers, libri)
- Applicazioni multilingua
- Massima qualit√† retrieval
- Backend server (non browser)

---

## Comparazione Veloce

| Modello | Dimensioni | Velocit√† Browser | Qualit√† | Contesto | Multilingual |
|---------|-----------|------------------|---------|----------|--------------|
| **all-MiniLM-L6-v2** | 90 MB | ‚ö°‚ö°‚ö° Molto Veloce | ‚≠ê‚≠ê‚≠ê Buona | 128 | ‚ùå |
| **bge-small-en-v1.5** | 133 MB | ‚ö°‚ö° Veloce | ‚≠ê‚≠ê‚≠ê‚≠ê Eccellente | 512 | ‚ùå |
| **nomic-embed-text-v1.5** | 270 MB | ‚ö° Moderata | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Top | 8192 | ‚úÖ |

---

## Raccomandazione per graphrag-wasm

**Usa: all-MiniLM-L6-v2 (Xenova)**

Motivi:
1. ‚úÖ Gi√† compatibile con il nostro tokenizer.json
2. ‚úÖ Dimensioni ottimali per browser (90 MB)
3. ‚úÖ Velocit√† eccellente per real-time interaction
4. ‚úÖ Qualit√† pi√π che sufficiente per la maggior parte dei casi GraphRAG
5. ‚úÖ Ampio testing community in ambiente WASM

**Upgrade path:**
- Per qualit√† superiore: bge-small-en-v1.5 (+47% dimensioni, +30% qualit√†)
- Per multilingual: nomic-embed-text-v1.5 (+200% dimensioni, +100% lingue)

---

## Come scaricare

```bash
# MiniLM-L6-v2 (consigliato)
cd /home/dio/graphrag-rs/graphrag-wasm
mkdir -p models
curl -L -o models/minilm-l6.onnx https://huggingface.co/Xenova/all-MiniLM-L6-v2/resolve/main/onnx/model.onnx

# bge-small-en-v1.5 (alternativa qualit√†)
curl -L -o models/bge-small.onnx https://huggingface.co/Xenova/bge-small-en-v1.5/resolve/main/onnx/model.onnx

# nomic-embed-text-v1.5 (multilingual)
curl -L -o models/nomic-embed.onnx https://huggingface.co/nomic-ai/nomic-embed-text-v1.5/resolve/main/onnx/model.onnx
```

---

## Performance Benchmark (stimato)

### Browser (WebAssembly + WebGPU)

| Modello | Tempo/embedding | Throughput | RAM Usage |
|---------|----------------|------------|-----------|
| MiniLM-L6 | ~3-5ms | 200-300 emb/s | ~150 MB |
| bge-small | ~8-12ms | 80-120 emb/s | ~250 MB |
| nomic-embed | ~20-30ms | 30-50 emb/s | ~450 MB |

*Note: Con WebGPU enabled. CPU-only sarebbe 10-20x pi√π lento.*

---

## Sources

- Modal Blog: Top embedding models for RAG (2024)
- DataStax: Best Embedding Models for Information Retrieval (2025)
- TigerData: Finding the Best Open-Source Embedding Model for RAG
- HuggingFace Model Hub: Official ONNX repositories
- Semantic Kernel: Local RAG implementations

**Data ricerca**: 2025-10-07
