{
  // GraphRAG Hybrid Pipeline Configuration
  // Combined semantic + algorithmic approach (Best of Both Worlds)
  // VSCode: This file has autocomplete! Type to see suggestions.

  "$schema": "../schema/graphrag-config.schema.json",

  // =========================================================================
  // MODE: Pipeline Approach Selection
  // =========================================================================
  mode: {
    approach: "hybrid"  // Options: "semantic", "algorithmic", "hybrid"
  },

  // =========================================================================
  // GENERAL: Basic Configuration
  // =========================================================================
  general: {
    input_document_path: "data/input.txt",
    output_dir: "./output/hybrid",
    log_level: "info",  // trace, debug, info, warn, error
    max_threads: 4,
    enable_profiling: true
  },

  // =========================================================================
  // HYBRID PIPELINE: Combined Semantic + Algorithmic
  // =========================================================================
  hybrid: {
    // --- Weights: Balance between Approaches ---
    weights: {
      semantic_weight: 0.6,      // 60% semantic (quality)
      algorithmic_weight: 0.4,   // 40% algorithmic (speed)

      // Dynamic weighting based on query characteristics
      use_dynamic_weighting: true,
      query_length_threshold: 10,  // Use more semantic for longer queries

      // Confidence-based weighting
      use_confidence_weighting: true,
      min_confidence_delta: 0.15
    },

    // --- Embeddings: Dual Strategy ---
    embeddings: {
      // Primary (semantic - high quality)
      primary_backend: "huggingface",
      primary_model: "sentence-transformers/all-MiniLM-L6-v2",
      primary_dimensions: 384,

      // Secondary (algorithmic - fast fallback)
      secondary_backend: "hash",
      secondary_hash_size: 1024,
      secondary_use_tfidf: true,

      // Fusion
      fusion_strategy: "weighted",  // Options: weighted, concat, max_pool
      normalize_before_fusion: true
    },

    // --- Entity Extraction: LLM + Pattern Fusion ---
    entity_extraction: {
      use_gleaning: true,   // ‚úÖ Semantic: LLM-based
      use_patterns: true,   // ‚úÖ Algorithmic: Pattern-based
      max_gleaning_rounds: 2,  // Reduced for efficiency

      // LLM configuration (semantic)
      llm_model: "llama3.1:8b",
      llm_temperature: 0.15,
      llm_max_tokens: 1000,

      // Pattern configuration (algorithmic)
      extract_capitalized: true,
      extract_acronyms: true,
      min_pattern_confidence: 0.5,

      // Entity fusion
      entity_fusion_strategy: "union",  // Options: union, intersection, weighted
      require_cross_validation: false,  // If true, require both methods
      cross_validation_boost: 1.2,      // Confidence boost if both agree

      confidence_threshold: 0.6,
      min_entity_length: 2,
      max_entity_length: 80,

      entity_types: [
        "PERSON",
        "ORGANIZATION",
        "LOCATION",
        "CONCEPT",
        "EVENT",
        "PRODUCT"
      ],

      use_semantic_merging: true,
      merge_similarity_threshold: 0.85
    },

    // --- Retrieval: RRF (Reciprocal Rank Fusion) ---
    retrieval: {
      fusion_strategy: "rrf",  // Options: rrf, weighted, cascade
      top_k: 10,  // Final number of results

      // RRF parameters
      rrf_k: 60,  // 20-100, typical: 60
      rrf_weights: [0.6, 0.4],  // [semantic, algorithmic]

      // Semantic retrieval settings
      semantic_strategy: "vector_similarity",
      semantic_top_k: 15,  // Retrieve more for fusion
      use_hnsw_index: true,
      hnsw_ef_construction: 150,
      hnsw_m: 16,
      min_semantic_similarity: 0.55,

      // Algorithmic retrieval settings
      algorithmic_strategy: "bm25",
      algorithmic_top_k: 15,
      bm25_k1: 1.5,
      bm25_b: 0.75,

      // Diversity and deduplication
      use_mmr: true,
      mmr_lambda: 0.7,
      deduplicate_results: true,
      dedup_similarity_threshold: 0.9
    },

    // --- Graph: Cross-validated Construction ---
    graph: {
      extract_relationships: true,
      relationship_confidence_threshold: 0.45,

      // Semantic relationships (LLM-based)
      use_semantic_relations: true,
      semantic_relation_weight: 0.6,

      // Algorithmic relationships (co-occurrence)
      use_cooccurrence: true,
      cooccurrence_window: 50,
      algorithmic_relation_weight: 0.4,

      // Relationship fusion
      require_cross_validation: false,
      cross_validation_boost: 1.3,

      // Graph scoring
      use_pagerank: true,
      pagerank_damping: 0.85,
      pagerank_iterations: 50,

      min_relation_score: 0.35,
      max_connections_per_node: 20,
      bidirectional_relations: true
    }
  },

  // =========================================================================
  // TEXT PROCESSING: Chunking and Enrichment
  // =========================================================================
  text_processing: {
    enabled: true,
    chunk_size: 512,
    chunk_overlap: 100,  // Balanced overlap
    min_chunk_size: 100,
    max_chunk_size: 1024,
    normalize_whitespace: true,
    remove_artifacts: true,

    enrichment: {
      enabled: true,
      auto_detect_format: true,
      parser_type: "auto",

      // Keyword extraction (algorithmic - TF-IDF)
      extract_keywords: true,
      max_keywords_per_chunk: 5,
      use_tfidf: true,

      // Summarization (semantic - could be LLM-enhanced)
      generate_summaries: true,
      min_chunk_length_for_summary: 150,
      max_summary_length: 120,

      extract_chapter: true,
      extract_section: true,
      extract_position: true,
      calculate_confidence: true
    }
  },

  // =========================================================================
  // OLLAMA: LLM Service (REQUIRED for semantic components)
  // =========================================================================
  ollama: {
    enabled: true,  // ‚úÖ Required for semantic components
    host: "http://localhost",
    port: 11434,
    chat_model: "llama3.1:8b",
    embedding_model: "nomic-embed-text",
    timeout_seconds: 45,
    max_retries: 3,
    fallback_to_hash: true,  // Fallback to algorithmic if unavailable

    generation: {
      temperature: 0.2,
      top_p: 0.9,
      max_tokens: 800,
      stream: false
    }
  },

  // =========================================================================
  // QUERY PROCESSING: Hybrid Query Understanding
  // =========================================================================
  query_processing: {
    enabled: true,
    use_advanced_pipeline: true,
    use_intent_classification: true,
    use_concept_extraction: true,
    confidence_threshold: 0.5,

    // Query routing (adaptive approach selection)
    use_query_routing: true,
    route_threshold: 0.7,
    route_short_queries_to_algorithmic: true,
    route_semantic_queries_to_neural: true,
    route_complex_queries_to_hybrid: true,

    intent_classification: {
      entity_patterns: ["who is", "what is", "person", "organization"],
      concept_patterns: ["concept", "idea", "definition", "similar to"],
      relationship_patterns: ["relationship", "connection", "related to"],
      search_patterns: ["find", "search", "locate"]
    },

    prompt_templates: {
      entity: "Based on the following context (from semantic and keyword analysis), provide information about: {query}\n\nContext:\n{context}\n\nAnswer:",
      concept: "Based on the following context (from semantic and keyword analysis), explain the concept: {query}\n\nContext:\n{context}\n\nExplanation:",
      relationship: "Based on the following context (from semantic and keyword analysis), describe relationships for: {query}\n\nContext:\n{context}\n\nRelationships:"
    }
  },

  // =========================================================================
  // PERFORMANCE: Balanced Resource Management
  // =========================================================================
  performance: {
    batch_processing: true,
    batch_size: 32,  // Balanced batch size
    worker_threads: 4,
    memory_limit_mb: 3072,  // Moderate memory usage
    cache_embeddings: true,

    // Selective caching
    cache_semantic_embeddings: true,   // Cache expensive
    cache_algorithmic_embeddings: false  // Cheap to recompute
  },

  // =========================================================================
  // MONITORING: Comprehensive Quality Tracking
  // =========================================================================
  monitoring: {
    enabled: true,
    track_entity_consistency: true,
    track_relationship_accuracy: true,
    track_fusion_quality: true,  // Track hybrid fusion quality
    log_insights: true,

    // Approach comparison
    compare_semantic_vs_algorithmic: true,
    log_approach_agreement: true
  }
}

/*
 * ============================================================================
 * HYBRID PIPELINE SUMMARY
 * ============================================================================
 *
 * ‚úÖ STRENGTHS:
 * - Best of both worlds: semantic quality + algorithmic speed
 * - Robust to individual approach failures (cross-validation)
 * - Better coverage (union of both extraction methods)
 * - Adaptive weighting based on query characteristics
 * - Graceful degradation (falls back if LLM fails)
 *
 * üéØ OPTIMAL USE CASES:
 * - Production systems requiring quality AND performance
 * - Diverse query workloads (short keywords + complex semantic)
 * - Systems with variable resource availability
 * - Applications requiring high recall
 * - Mixed document types (structured + unstructured)
 *
 * ‚öñÔ∏è  TRADE-OFFS:
 * - Speed: 2-3x faster than pure semantic
 * - Quality: 90-95% of pure semantic quality
 * - Memory: 1.5x semantic memory usage
 * - Complexity: More parameters to tune
 *
 * üìä PERFORMANCE:
 * - Latency: ~100-200ms per query
 * - Throughput: 200-1000 documents/second
 * - Memory: 3-4GB
 * - Accuracy: 85-95%
 *
 * üéöÔ∏è  TUNING GUIDE:
 *
 * For QUALITY-first:
 *   semantic_weight: 0.7
 *   algorithmic_weight: 0.3
 *   max_gleaning_rounds: 3
 *   require_cross_validation: true
 *
 * For SPEED-first:
 *   semantic_weight: 0.4
 *   algorithmic_weight: 0.6
 *   max_gleaning_rounds: 1
 *   require_cross_validation: false
 *
 * For BALANCED (default):
 *   semantic_weight: 0.6
 *   algorithmic_weight: 0.4
 *   max_gleaning_rounds: 2
 *   require_cross_validation: false
 *
 * üöÄ QUICK START:
 * 1. Install Ollama: https://ollama.ai/
 * 2. Pull models: ollama pull llama3.1:8b && ollama pull nomic-embed-text
 * 3. Adjust weights based on quality/speed requirements
 * 4. Run: cargo run --example your_example -- hybrid.graphrag.json5
 * 5. Monitor fusion quality and adjust weights
 *
 * üí° ADVANCED OPTIMIZATION:
 * - Enable use_dynamic_weighting for adaptive routing
 * - Use query_routing to select best approach per query
 * - Tune rrf_k based on result distribution (40-80 typical)
 * - Enable cross_validation_boost for high-confidence requirements
 * ============================================================================
 */
