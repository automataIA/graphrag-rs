# =============================================================================
# ALGORITHMIC PIPELINE CONFIGURATION
# Classic NLP/Pattern-based approach for fast, resource-efficient processing
# =============================================================================
#
# This template demonstrates a PURE ALGORITHMIC pipeline using:
# - Hash-based embeddings with TF-IDF weighting
# - Pattern-based entity extraction (regex + capitalization rules)
# - Keyword-based retrieval (BM25, Jaccard similarity)
# - Deterministic graph construction without LLM inference
#
# Best for: Fast processing, resource-constrained environments, deterministic results
# Requires: No LLM or GPU (CPU-only, minimal dependencies)
# Resource usage: Low (runs efficiently on laptop CPUs)
#
# =============================================================================

# -----------------------------------------------------------------------------
# MODE: Pipeline Approach Selection
# -----------------------------------------------------------------------------
[mode]
# Choose pipeline approach: "semantic", "algorithmic", or "hybrid"
approach = "algorithmic"

# -----------------------------------------------------------------------------
# GENERAL: Basic Configuration
# -----------------------------------------------------------------------------
[general]
input_document_path = "data/input.txt"
output_dir = "./output/algorithmic"
log_level = "info"
max_threads = 4
enable_profiling = true

# -----------------------------------------------------------------------------
# ALGORITHMIC PIPELINE: Pattern/NLP-based Configuration
# -----------------------------------------------------------------------------
[algorithmic]

# --- ALGORITHMIC EMBEDDINGS: Hash-based with TF-IDF ---
[algorithmic.embeddings]
# Hash-based backend for fast, deterministic embeddings
backend = "hash"

# Hashing parameters
hash_size = 1024         # Number of hash buckets (higher = more precision)
use_tfidf_weighting = true  # Weight terms by TF-IDF importance
min_ngram = 1            # Minimum n-gram size
max_ngram = 2            # Maximum n-gram size (1-2 = unigrams + bigrams)
lowercase = true         # Normalize to lowercase
remove_stopwords = true  # Remove common stopwords

# Alternative: Simple word count embeddings
# backend = "count"
# vocabulary_size = 10000

# --- ALGORITHMIC ENTITY EXTRACTION: Pattern-based ---
[algorithmic.entity_extraction]
# Use pattern-based extraction (NO LLM required)
use_gleaning = false     # Disable LLM-based gleaning
use_patterns = true      # Enable pattern matching

# Pattern-based extraction settings
extract_capitalized = true     # Extract capitalized words (proper nouns)
extract_acronyms = true        # Extract acronyms (NASA, FBI, etc.)
extract_numbers = true         # Extract numeric entities
extract_emails = false         # Extract email addresses
extract_urls = false           # Extract URLs

# Confidence and filtering
confidence_threshold = 0.5  # Lower threshold for pattern-based extraction
min_entity_length = 2
max_entity_length = 50      # Shorter max for efficiency

# Entity type patterns (regex-based)
entity_patterns = [
    # Organizations (all caps or Title Case followed by Inc/Corp/Ltd)
    { type = "ORGANIZATION", pattern = "(?:[A-Z][a-z]+\\s+)*(?:Inc|Corp|Ltd|LLC|Company|Group)" },

    # Locations (Title Case + geographic keywords)
    { type = "LOCATION", pattern = "(?:[A-Z][a-z]+\\s+)*(?:City|State|County|Country|River|Mountain)" },

    # Products (Title Case + version numbers)
    { type = "PRODUCT", pattern = "[A-Z][a-zA-Z0-9]+(?:\\s+[A-Z0-9][a-zA-Z0-9]*)*\\s+(?:v?\\d+\\.\\d+|\\d{4})" },

    # Dates (various formats)
    { type = "DATE", pattern = "\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}" },

    # Money amounts
    { type = "MONEY", pattern = "\\$\\d+(?:,\\d{3})*(?:\\.\\d{2})?|\\d+(?:,\\d{3})*(?:\\.\\d{2})?\\s*(?:USD|EUR|GBP)" }
]

# Stopword filtering (exclude common words)
exclude_stopwords = true
custom_stopwords = ["the", "and", "but", "for", "with", "from", "this", "that"]

# --- ALGORITHMIC RETRIEVAL: BM25 Keyword Search ---
[algorithmic.retrieval]
# Retrieval strategy
strategy = "bm25"        # BM25 keyword-based retrieval
top_k = 10               # Number of results to retrieve

# BM25 parameters (Okapi BM25)
bm25_k1 = 1.5            # Term frequency saturation (typical: 1.2-2.0)
bm25_b = 0.75            # Length normalization (typical: 0.75)
bm25_epsilon = 0.25      # Floor value for IDF (prevents zero weights)

# Alternative: TF-IDF retrieval
# strategy = "tfidf"
# tfidf_norm = "l2"      # Normalization: "l1", "l2", or "max"

# Alternative: Jaccard similarity
# strategy = "jaccard"
# jaccard_threshold = 0.3

# Filtering
min_keyword_score = 0.1  # Minimum keyword relevance
use_query_expansion = false  # Disable LLM-based query expansion

# --- ALGORITHMIC GRAPH: Deterministic Graph Construction ---
[algorithmic.graph]
# Deterministic relationship extraction
extract_relationships = true
relationship_confidence_threshold = 0.4

# Co-occurrence based relationships
use_cooccurrence = true
cooccurrence_window = 50    # Word window for co-occurrence
min_cooccurrence_count = 2  # Minimum co-occurrences

# Graph scoring (without PageRank - simpler, faster)
use_pagerank = false
use_degree_centrality = true  # Use simple degree centrality

# Graph connectivity
min_relation_score = 0.3
max_connections_per_node = 15
bidirectional_relations = true

# -----------------------------------------------------------------------------
# TEXT PROCESSING: Chunking and Enrichment
# -----------------------------------------------------------------------------
[text_processing]
enabled = true
chunk_size = 512
chunk_overlap = 64         # Smaller overlap for speed
min_chunk_size = 100
max_chunk_size = 1024
normalize_whitespace = true
remove_artifacts = true

# Text enrichment (structure detection, keywords, summaries)
[text_processing.enrichment]
enabled = true
auto_detect_format = true
parser_type = "auto"

# Keyword extraction (TF-IDF - NO LLM)
extract_keywords = true
max_keywords_per_chunk = 5
use_tfidf = true          # Real TF-IDF algorithm

# Summarization (extractive - NO LLM)
generate_summaries = true
min_chunk_length_for_summary = 150
max_summary_length = 100   # Shorter summaries for speed

# Metadata extraction
extract_chapter = true
extract_section = true
extract_position = true
calculate_confidence = true

# -----------------------------------------------------------------------------
# OLLAMA: LLM Service (DISABLED for algorithmic pipeline)
# -----------------------------------------------------------------------------
[ollama]
enabled = false           # ‚ùå LLM NOT required for algorithmic pipeline
host = "http://localhost"
port = 11434
timeout_seconds = 30
max_retries = 1
fallback_to_hash = true   # Fallback to hash embeddings if Ollama unavailable

# -----------------------------------------------------------------------------
# QUERY PROCESSING: Pattern-based Query Understanding
# -----------------------------------------------------------------------------
[query_processing]
enabled = true
use_advanced_pipeline = false  # Disable LLM-based advanced processing
use_intent_classification = true
use_concept_extraction = false # Disable LLM-based concept extraction
confidence_threshold = 0.4

[query_processing.intent_classification]
# Intent patterns (rule-based, no LLM)
entity_patterns = ["who is", "what is", "person", "organization", "company"]
concept_patterns = ["concept", "idea", "definition", "meaning", "explain"]
relationship_patterns = ["relationship", "connection", "related to", "associated with"]
search_patterns = ["find", "search", "look for", "locate"]

[query_processing.prompt_templates]
# Simple templates (no LLM generation - just context retrieval)
entity = """Query: {query}

Relevant information:
{context}"""

concept = """Query: {query}

Relevant information:
{context}"""

relationship = """Query: {query}

Relevant relationships:
{context}"""

# -----------------------------------------------------------------------------
# PERFORMANCE: Optimized for Speed
# -----------------------------------------------------------------------------
[performance]
batch_processing = true
batch_size = 64           # Larger batches for CPU efficiency
worker_threads = 4
memory_limit_mb = 2048    # Lower memory footprint
cache_embeddings = true

# -----------------------------------------------------------------------------
# MONITORING: Basic Metrics
# -----------------------------------------------------------------------------
[monitoring]
enabled = true
track_entity_consistency = true
track_relationship_accuracy = false  # Simpler monitoring
log_insights = false

# =============================================================================
# ALGORITHMIC PIPELINE SUMMARY:
#
# ‚úÖ STRENGTHS:
# - Fast processing speed (no neural network inference)
# - Low resource requirements (CPU-only, minimal memory)
# - Deterministic and reproducible results
# - No external API dependencies (no OpenAI, no Ollama)
# - Works well for structured data and clear patterns
# - Excellent for exact keyword matching and known entities
#
# ‚ö†Ô∏è  CONSIDERATIONS:
# - Lower quality for nuanced/implicit relationships
# - Struggles with synonyms and paraphrasing
# - Pattern-based extraction may miss context-dependent entities
# - Limited understanding of semantic similarity
# - Requires good pattern engineering for domain-specific terms
#
# üéØ BEST USE CASES:
# - Large-scale document processing (millions of documents)
# - Resource-constrained environments (edge devices, CI/CD)
# - Real-time/low-latency applications
# - Structured data with clear entity patterns
# - Privacy-sensitive applications (no external API calls)
# - Batch processing pipelines
#
# ‚ö° PERFORMANCE COMPARISON:
# - Speed: ~10-50x faster than semantic pipeline
# - Memory: ~5-10x less memory usage
# - Accuracy: 70-85% of semantic quality (domain-dependent)
#
# üöÄ QUICK START:
# 1. No installation required (pure Rust, no dependencies)
# 2. Customize entity_patterns for your domain
# 3. Run pipeline: cargo run --example your_example -- algorithmic_pipeline.toml
# 4. Scale horizontally for massive datasets
#
# üí° OPTIMIZATION TIPS:
# - Tune hash_size based on vocabulary size (1024-4096)
# - Adjust bm25_k1 for your document length distribution
# - Add domain-specific patterns to entity_patterns
# - Use larger batch_size for CPU-bound workloads
# - Enable parallel processing for multi-core systems
#
# =============================================================================
