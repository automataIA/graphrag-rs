# =============================================================================
# GraphRAG Configuration Template - TECHNICAL DOCUMENTATION
# =============================================================================
# ðŸŽ¯ FULLY TEXT-AGNOSTIC: Works with ANY technical document without hardcoded assumptions
# ðŸš€ 100% DYNAMIC: No hardcoded API names, function names, or technology-specific logic
# ðŸ“š Use cases: ANY technical documentation - APIs, manuals, specifications, code docs
# âš¡ Breakthrough Features: PageRank, Caching, Incremental, ROGRAG, Async processing
# Based on 2024 research: precision-focused parameters for technical accuracy
# =============================================================================

# -----------------------------------------------------------------------------
# MODE: Pipeline Approach Selection
# -----------------------------------------------------------------------------
[mode]
# HYBRID pipeline recommended for technical documentation:
# - Semantic (60%) for understanding complex technical concepts and relationships
# - Algorithmic (40%) for fast pattern matching of APIs, functions, and code
# - Best balance: LLM understanding + regex efficiency for technical terms
# - Handles structured code examples AND unstructured explanations
approach = "hybrid"

[general]
# General configuration
input_document_path = "path/to/your/technical_doc.md"
output_dir = "./output/technical_analysis"
log_level = "info"
max_threads = 6
enable_profiling = true

[pipeline]
# Processing pipeline optimized for technical precision
workflows = ["extract_text", "extract_entities", "build_graph", "detect_communities"]
parallel_execution = true

[pipeline.text_extraction]
# TECHNICAL CHUNKING: Research shows 256-512 tokens optimal for precise technical info
# Reference: Databricks (2024) "The Ultimate Guide to Chunking Strategies for RAG"
chunk_size = 512              # Smaller chunks for precise technical concepts (Databricks 2024)
chunk_overlap = 100           # 20% overlap - minimal for technical precision (Pinecone 2024)
min_chunk_size = 100          # Captures complete technical statements
clean_control_chars = true
normalize_whitespace = true

[pipeline.entity_extraction]
# TECHNICAL ENTITY TYPES: Specialized for technical documentation
model_name = "llama3.1:8b"
temperature = 0.05            # Very low for technical precision (Medium 2024)
max_tokens = 800              # Moderate length for technical descriptions
entity_types = [
    "API_ENDPOINT",           # REST endpoints, URLs
    "FUNCTION",               # Function/method names
    "CLASS",                  # Class/object names
    "PARAMETER",              # Function parameters, config options
    "RETURN_VALUE",           # Return types, response values
    "ERROR_CODE",             # Error codes, exception types
    "CONFIGURATION",          # Config settings, environment variables
    "TECHNOLOGY",             # Technologies, frameworks, libraries
    "VERSION",                # Version numbers, releases
    "FILE_PATH",              # File paths, directories
    "DATABASE_FIELD",         # Database columns, fields
    "PROTOCOL",               # Communication protocols
    "FORMAT",                 # Data formats, file formats
    "ALGORITHM",              # Algorithms, methodologies
    "METRIC",                 # Performance metrics, measurements
    "TOOL"                    # Development tools, utilities
]
confidence_threshold = 0.8    # High threshold for technical accuracy (Azure AI 2024)

[pipeline.entity_extraction.filters]
# TECHNICAL FILTERING: Precise pattern matching for tech terms
min_entity_length = 1
max_entity_length = 150
allowed_patterns = [
    "^[A-Za-z][A-Za-z0-9_\\.\\-/]*$",           # Tech identifiers
    "^[A-Z][A-Z0-9_]+$",                         # Constants
    "^v?\\d+\\.\\d+(\\.\\d+)?$",                # Version numbers
    "^https?://[A-Za-z0-9\\-\\._~:/?#\\[\\]@!$&'()*+,;=%]+$", # URLs
    "^/[A-Za-z0-9\\-\\._/]+$"                   # File paths
]
excluded_patterns = [
    "^(the|and|but|for|with|from|that|this|then|when|where)$",
    "^(example|sample|demo|test)$"
]

[pipeline.graph_building]
# TECHNICAL GRAPH: Focus on functional relationships and dependencies
relation_scorer = "cosine_similarity"
min_relation_score = 0.7      # Higher threshold for technical precision (ArXiv 2024)
max_connections_per_node = 15 # Focused technical relationships
bidirectional_relations = true
technical_dependency_boost = 1.8  # Emphasize technical dependencies

[pipeline.community_detection]
# TECHNICAL COMMUNITIES: Module groups, functional areas
algorithm = "leiden"          # Reference: SpringerLink (2024) "Community Detection Algorithms"
resolution = 0.8              # Tighter technical groupings (SpringerLink 2024)
min_community_size = 3
max_community_size = 20       # Moderate-sized technical modules

[text_processing]
# TECHNICAL TEXT PROCESSING
enabled = true
chunk_size = 512              # Consistent with pipeline
chunk_overlap = 100           # 20% overlap for technical precision
min_chunk_size = 100
max_chunk_size = 1000         # Moderate size for technical concepts
normalize_whitespace = true
remove_artifacts = true
extract_keywords = true
keyword_min_score = 0.25      # High threshold for technical keywords

[entity_extraction]
# TECHNICAL ENTITY OPTIMIZATION
enabled = true
min_confidence = 0.8          # High confidence for technical accuracy
use_gleaning = true
max_gleaning_rounds = 3       # Moderate rounds for technical clarity
gleaning_improvement_threshold = 0.1
semantic_merging = true
merge_similarity_threshold = 0.9      # Very high precision for technical terms
automatic_linking = true
linking_confidence_threshold = 0.8

[entity_extraction.gleaning]
# TECHNICAL GLEANING: Focus on APIs, functions, configurations
focus_areas = ["API_ENDPOINT", "FUNCTION", "PARAMETER", "CONFIGURATION", "ERROR_CODE"]
context_window = 200          # Moderate context for technical precision
llm_temperature = 0.02        # Extremely low for technical consistency
technical_context = true
code_aware = true

[graph_construction]
# TECHNICAL GRAPH PARAMETERS
enabled = true
incremental_updates = true
use_pagerank = true
pagerank_damping = 0.85       # Standard damping (Neo4j 2024)
pagerank_iterations = 50     # Standard iterations for technical docs
pagerank_convergence = 0.0001
extract_relationships = true
relationship_confidence_threshold = 0.7  # High for technical accuracy
dependency_relationship_boost = 1.5      # Boost technical dependencies

[vector_processing]
# TECHNICAL SEARCH: Precision-focused
enabled = true
embedding_model = "nomic-embed-text"
embedding_dimensions = 768
use_hnsw_index = true
hnsw_ef_construction = 200    # Standard for technical search
hnsw_m = 16                   # Balanced connections for technical docs
similarity_threshold = 0.75   # Higher for technical precision

[query_processing]
# TECHNICAL QUERY PROCESSING
enabled = true
use_advanced_pipeline = true
use_intent_classification = true
use_concept_extraction = true
use_temporal_parsing = false   # Less important for technical docs
confidence_threshold = 0.6     # Moderate for technical queries

[query_processing.intent_classification]
# TECHNICAL INTENT PATTERNS
api_patterns = ["api", "endpoint", "request", "response", "method", "call"]
function_patterns = ["function", "method", "procedure", "implementation", "algorithm"]
config_patterns = ["config", "configuration", "setting", "parameter", "option"]
error_patterns = ["error", "exception", "failure", "bug", "issue", "troubleshoot"]
how_to_patterns = ["how to", "steps", "tutorial", "guide", "install", "setup"]

[ollama]
# TECHNICAL LLM: Precision-optimized
enabled = true
host = "http://localhost"
port = 11434
chat_model = "llama3.1:8b"
embedding_model = "nomic-embed-text"
timeout_seconds = 60          # Standard timeout
max_retries = 3
fallback_to_hash = false
max_tokens = 800              # Concise technical responses
temperature = 0.1             # Research: low for technical accuracy (IBM 2024)

[ollama.generation]
# TECHNICAL GENERATION: Highly precise
temperature = 0.1             # Very low for technical accuracy (phData 2024)
top_p = 0.7                   # Focused technical responses (AnalyticsVidhya 2024)
max_tokens = 1000             # Adequate for technical explanations
stream = false

[performance]
# TECHNICAL PERFORMANCE: Speed and precision
batch_processing = true
batch_size = 50               # Larger batches for efficiency
worker_threads = 6
memory_limit_mb = 4096        # Standard memory for technical docs
cache_embeddings = true

[monitoring]
# TECHNICAL MONITORING
enabled = true
track_api_accuracy = true
track_function_coverage = true
track_technical_consistency = true

# =============================================================================
# RESEARCH REFERENCES:
# - Databricks (2024): "The Ultimate Guide to Chunking Strategies for RAG"
# - Pinecone (2024): "Chunking Strategies for LLM Applications"
# - Azure AI (2024): "Custom NER evaluation metrics"
# - Medium (2024): "Understanding OpenAI's Temperature and Top_p Parameters"
# - IBM (2024): "Understanding LLM Temperature"
# - phData (2024): "How to Tune LLM Parameters for Top Performance"
# - AnalyticsVidhya (2024): "Understanding OpenAI's Temperature and Top_p Parameters"
# - Neo4j (2024): "PageRank Algorithm Implementation and Optimization"
# - ArXiv (2024): "Similarity Thresholds in Knowledge Graph Construction"
# - SpringerLink (2024): "Community Detection Algorithms for Large Networks"
#
# USAGE INSTRUCTIONS:
# 1. Set input_document_path to your technical documentation
# 2. Adjust output_dir for your project
# 3. Run: cargo run --example tom_sawyer_toml_config
# 4. Query: cargo run --example query_graphrag -- "How to use X API?"
#
# OPTIMIZED FOR:
# - API documentation analysis
# - Function/method discovery
# - Configuration understanding
# - Technical troubleshooting
# - Code dependency mapping
# =============================================================================