# =============================================================================
# HYBRID PIPELINE CONFIGURATION
# Combined semantic + algorithmic approach for balanced quality and performance
# =============================================================================
#
# This template demonstrates a HYBRID pipeline combining:
# - Semantic (neural/LLM-based) for quality and nuance
# - Algorithmic (pattern-based) for speed and coverage
# - Weighted fusion using RRF (Reciprocal Rank Fusion)
# - Cross-validation between approaches for confidence scoring
#
# Best for: Production systems requiring both quality and efficiency
# Requires: Ollama (or OpenAI API) for semantic components
# Resource usage: Medium (balanced between semantic and algorithmic)
#
# =============================================================================

# -----------------------------------------------------------------------------
# MODE: Pipeline Approach Selection
# -----------------------------------------------------------------------------
[mode]
# Choose pipeline approach: "semantic", "algorithmic", or "hybrid"
approach = "hybrid"

# -----------------------------------------------------------------------------
# GENERAL: Basic Configuration
# -----------------------------------------------------------------------------
[general]
input_document_path = "data/input.txt"
output_dir = "./output/hybrid"
log_level = "info"
max_threads = 4
enable_profiling = true

# -----------------------------------------------------------------------------
# HYBRID PIPELINE: Combined Semantic + Algorithmic Configuration
# -----------------------------------------------------------------------------
[hybrid]

# --- HYBRID WEIGHTS: Balance between Approaches ---
[hybrid.weights]
# Weight distribution (must sum to 1.0)
semantic_weight = 0.6      # 60% weight for semantic/neural results
algorithmic_weight = 0.4   # 40% weight for algorithmic/pattern results

# Dynamic weighting based on query characteristics
use_dynamic_weighting = true
query_length_threshold = 10  # Use more semantic for longer queries

# Confidence-based weighting
use_confidence_weighting = true
min_confidence_delta = 0.15  # Minimum confidence difference for reweighting

# --- HYBRID EMBEDDINGS: Dual Embedding Strategy ---
[hybrid.embeddings]
# Primary embedding (semantic - high quality)
primary_backend = "huggingface"
primary_model = "sentence-transformers/all-MiniLM-L6-v2"
primary_dimensions = 384

# Secondary embedding (algorithmic - fast fallback)
secondary_backend = "hash"
secondary_hash_size = 1024
secondary_use_tfidf = true

# Embedding fusion
fusion_strategy = "weighted"  # Options: "weighted", "concat", "max_pool"
normalize_before_fusion = true

# --- HYBRID ENTITY EXTRACTION: LLM + Pattern Fusion ---
[hybrid.entity_extraction]
# Enable both extraction methods
use_gleaning = true          # Semantic: LLM-based extraction
use_patterns = true          # Algorithmic: Pattern-based extraction
max_gleaning_rounds = 2      # Reduced rounds for efficiency

# LLM configuration (for semantic extraction)
llm_model = "llama3.1:8b"
llm_temperature = 0.15
llm_max_tokens = 1000

# Pattern configuration (for algorithmic extraction)
extract_capitalized = true
extract_acronyms = true
min_pattern_confidence = 0.5

# Entity fusion strategy
entity_fusion_strategy = "union"  # Options: "union", "intersection", "weighted"
require_cross_validation = false   # If true, require both methods to agree
cross_validation_boost = 1.2       # Boost confidence if both methods agree

# Confidence thresholds
confidence_threshold = 0.6
min_entity_length = 2
max_entity_length = 80

# Entity types (used by both methods)
entity_types = [
    "PERSON",
    "ORGANIZATION",
    "LOCATION",
    "CONCEPT",
    "EVENT",
    "PRODUCT"
]

# Semantic merging (uses semantic embeddings)
use_semantic_merging = true
merge_similarity_threshold = 0.85

# --- HYBRID RETRIEVAL: RRF Fusion ---
[hybrid.retrieval]
# Retrieval fusion strategy
fusion_strategy = "rrf"      # Reciprocal Rank Fusion (RRF)
top_k = 10                   # Final number of results

# RRF parameters (see: https://arxiv.org/abs/2106.11757)
rrf_k = 60                   # RRF constant (typical: 60)
rrf_weights = [0.6, 0.4]     # [semantic_weight, algorithmic_weight]

# Semantic retrieval settings
semantic_strategy = "vector_similarity"
semantic_top_k = 15          # Retrieve more for fusion
use_hnsw_index = true
hnsw_ef_construction = 150
hnsw_m = 16
min_semantic_similarity = 0.55

# Algorithmic retrieval settings
algorithmic_strategy = "bm25"
algorithmic_top_k = 15       # Retrieve more for fusion
bm25_k1 = 1.5
bm25_b = 0.75

# Diversity and deduplication
use_mmr = true               # Maximal Marginal Relevance
mmr_lambda = 0.7
deduplicate_results = true
dedup_similarity_threshold = 0.9

# --- HYBRID GRAPH: Cross-validated Graph Construction ---
[hybrid.graph]
# Relationship extraction from both approaches
extract_relationships = true
relationship_confidence_threshold = 0.45

# Semantic relationships (LLM-based)
use_semantic_relations = true
semantic_relation_weight = 0.6

# Algorithmic relationships (co-occurrence)
use_cooccurrence = true
cooccurrence_window = 50
algorithmic_relation_weight = 0.4

# Relationship fusion
require_cross_validation = false  # If true, require both methods
cross_validation_boost = 1.3

# Graph scoring
use_pagerank = true
pagerank_damping = 0.85
pagerank_iterations = 50

# Graph connectivity
min_relation_score = 0.35
max_connections_per_node = 20
bidirectional_relations = true

# -----------------------------------------------------------------------------
# TEXT PROCESSING: Chunking and Enrichment
# -----------------------------------------------------------------------------
[text_processing]
enabled = true
chunk_size = 512
chunk_overlap = 100          # Balanced overlap
min_chunk_size = 100
max_chunk_size = 1024
normalize_whitespace = true
remove_artifacts = true

# Text enrichment
[text_processing.enrichment]
enabled = true
auto_detect_format = true
parser_type = "auto"

# Keyword extraction (algorithmic - TF-IDF)
extract_keywords = true
max_keywords_per_chunk = 5
use_tfidf = true

# Summarization (semantic - extractive, could be LLM-enhanced)
generate_summaries = true
min_chunk_length_for_summary = 150
max_summary_length = 120

# Metadata extraction
extract_chapter = true
extract_section = true
extract_position = true
calculate_confidence = true

# -----------------------------------------------------------------------------
# OLLAMA: LLM Service (for semantic components)
# -----------------------------------------------------------------------------
[ollama]
enabled = true               # ‚úÖ Required for semantic components
host = "http://localhost"
port = 11434
chat_model = "llama3.1:8b"
embedding_model = "nomic-embed-text"
timeout_seconds = 45
max_retries = 3
fallback_to_hash = true      # Fallback to algorithmic if LLM unavailable

[ollama.generation]
temperature = 0.2
top_p = 0.9
max_tokens = 800
stream = false

# -----------------------------------------------------------------------------
# QUERY PROCESSING: Hybrid Query Understanding
# -----------------------------------------------------------------------------
[query_processing]
enabled = true
use_advanced_pipeline = true
use_intent_classification = true
use_concept_extraction = true
confidence_threshold = 0.5

# Query routing (adaptive approach selection)
use_query_routing = true
route_threshold = 0.7        # Confidence threshold for routing

# Routing rules
route_short_queries_to_algorithmic = true   # Short queries ‚Üí BM25
route_semantic_queries_to_neural = true     # "Similar to..." ‚Üí vector search
route_complex_queries_to_hybrid = true      # Complex ‚Üí use both

[query_processing.intent_classification]
entity_patterns = ["who is", "what is", "person", "organization"]
concept_patterns = ["concept", "idea", "definition", "meaning", "similar to"]
relationship_patterns = ["relationship", "connection", "related to"]
search_patterns = ["find", "search", "locate"]

[query_processing.prompt_templates]
entity = """Based on the following context (from semantic and keyword analysis), provide information about: {query}

Context:
{context}

Answer:"""

concept = """Based on the following context (from semantic and keyword analysis), explain the concept: {query}

Context:
{context}

Explanation:"""

relationship = """Based on the following context (from semantic and keyword analysis), describe the relationships for: {query}

Context:
{context}

Relationships:"""

# -----------------------------------------------------------------------------
# PERFORMANCE: Balanced Resource Management
# -----------------------------------------------------------------------------
[performance]
batch_processing = true
batch_size = 32              # Balanced batch size
worker_threads = 4
memory_limit_mb = 3072       # Moderate memory usage
cache_embeddings = true

# Selective caching (cache expensive semantic embeddings)
cache_semantic_embeddings = true
cache_algorithmic_embeddings = false  # Hash embeddings are cheap to recompute

# -----------------------------------------------------------------------------
# MONITORING: Comprehensive Quality Tracking
# -----------------------------------------------------------------------------
[monitoring]
enabled = true
track_entity_consistency = true
track_relationship_accuracy = true
track_fusion_quality = true          # Track hybrid fusion quality
log_insights = true

# Approach comparison metrics
compare_semantic_vs_algorithmic = true
log_approach_agreement = true

# =============================================================================
# HYBRID PIPELINE SUMMARY:
#
# ‚úÖ STRENGTHS:
# - Best of both worlds: semantic quality + algorithmic speed
# - Robust to individual approach failures (cross-validation)
# - Better coverage (union of both extraction methods)
# - Adaptive weighting based on query characteristics
# - Confidence-boosting for cross-validated results
# - Graceful degradation (falls back to algorithmic if LLM fails)
#
# üéØ OPTIMAL USE CASES:
# - Production systems requiring quality AND performance
# - Diverse query workloads (short keywords + complex semantic queries)
# - Systems with variable resource availability
# - Applications requiring high recall (don't miss entities)
# - Mixed document types (structured + unstructured)
#
# ‚öñÔ∏è  TRADE-OFFS:
# - Speed: 2-3x faster than pure semantic (but slower than pure algorithmic)
# - Quality: 90-95% of pure semantic quality
# - Memory: 1.5x semantic memory usage
# - Complexity: More configuration parameters to tune
#
# üìä PERFORMANCE CHARACTERISTICS:
# - Latency: ~100-200ms per query (depends on top_k and fusion)
# - Throughput: ~100-500 documents/second (depends on gleaning rounds)
# - Memory: ~3-4GB for typical workloads
# - Accuracy: 85-95% (better than algorithmic, competitive with semantic)
#
# üéöÔ∏è  TUNING RECOMMENDATIONS:
#
# For QUALITY-first workloads:
#   semantic_weight = 0.7
#   algorithmic_weight = 0.3
#   max_gleaning_rounds = 3
#   require_cross_validation = true
#
# For SPEED-first workloads:
#   semantic_weight = 0.4
#   algorithmic_weight = 0.6
#   max_gleaning_rounds = 1
#   require_cross_validation = false
#
# For BALANCED workloads (default):
#   semantic_weight = 0.6
#   algorithmic_weight = 0.4
#   max_gleaning_rounds = 2
#   require_cross_validation = false
#
# üöÄ QUICK START:
# 1. Install Ollama: https://ollama.ai/
# 2. Pull models: ollama pull llama3.1:8b && ollama pull nomic-embed-text
# 3. Adjust weights based on your quality/speed requirements
# 4. Run pipeline: cargo run --example your_example -- hybrid_pipeline.toml
# 5. Monitor fusion quality metrics and adjust weights
#
# üí° ADVANCED OPTIMIZATION:
# - Enable use_dynamic_weighting for adaptive query routing
# - Use query_routing to automatically select best approach per query
# - Tune rrf_k based on your result distribution (40-80 typical)
# - Enable cross_validation_boost for high-confidence requirements
# - Adjust semantic_top_k and algorithmic_top_k for RRF fusion quality
#
# =============================================================================
