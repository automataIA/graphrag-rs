{
  // ==========================================================================
  // Symposium with LLM: Full Semantic Approach
  // ==========================================================================
  // üéØ APPROACH: LLM-based extraction with gleaning + semantic embeddings
  // üí∞ COST: ~$5-10 indexing, ~$0.50 per query (full LLM pipeline)
  // ‚è±Ô∏è TIME: 3-5 minutes processing, 2-3s per query
  // üéì USE CASE: Maximum quality philosophical analysis
  // ==========================================================================

  "$schema": "../schema/graphrag-config.schema.json",

  // ---------------------------------------------------------------------------
  // MODE: Semantic Pipeline (Full LLM)
  // ---------------------------------------------------------------------------
  mode: {
    approach: "semantic"  // Full semantic/neural pipeline with LLM
  },

  // ---------------------------------------------------------------------------
  // GENERAL SETTINGS
  // ---------------------------------------------------------------------------
  general: {
    input_document_path: "docs-example/Symposium.txt",
    output_dir: "./output/symposium_with_llm",
    log_level: "info",
    max_threads: 4,
    enable_profiling: true
  },

  // ---------------------------------------------------------------------------
  // SEMANTIC PIPELINE: Full LLM Components
  // ---------------------------------------------------------------------------
  semantic: {
    // Neural embeddings via Ollama
    embeddings: {
      backend: "ollama",                // Use Ollama for embeddings
      model_name: "nomic-embed-text",
      embedding_dimensions: 768,        // nomic-embed-text dimension
      normalize_embeddings: true
    },

    // LLM-based entity extraction with gleaning
    entity_extraction: {
      use_gleaning: false,
      max_gleaning_rounds: 2,           // Deep concept extraction

      llm_model: "qwen3:8b-q4_k_m",
      llm_temperature: 0.1,             // Low for consistent extraction
      llm_max_tokens: 1500,

      confidence_threshold: 0.65,
      min_entity_length: 2,
      max_entity_length: 100,

      // Philosophical entity types
      entity_types: [
        "PERSON",                       // Socrates, Aristophanes, etc.
        "CONCEPT",                      // Love, Beauty, Wisdom, Virtue
        "ARGUMENT",                     // Philosophical arguments
        "THEME",                        // Main themes
        "SPEAKER",                      // Dialogue speakers
        "MYTHOLOGICAL_REFERENCE",       // Gods, myths
        "EMOTION",                      // Emotional states
        "RELATIONSHIP",                 // Philosophical relationships
        "LOCATION",                     // Athens, symposium setting
        "EVENT",                        // Speeches, dialogues
        "OBJECT"                        // Symbols, metaphors
      ],

      use_semantic_merging: true,
      merge_similarity_threshold: 0.85
    },

    // Vector similarity retrieval with HNSW
    retrieval: {
      strategy: "vector_similarity",
      top_k: 10,

      use_hnsw_index: true,
      hnsw_ef_construction: 300,        // Higher for quality
      hnsw_m: 24,                       // More connections
      hnsw_ef_search: 50,

      min_similarity_score: 0.65,

      use_mmr: true,                    // Maximal Marginal Relevance
      mmr_lambda: 0.7                   // Balance relevance vs diversity
    },

    // Semantic graph construction
    graph_construction: {
      extract_relationships: true,
      relationship_confidence_threshold: 0.5,

      use_pagerank: true,
      pagerank_damping: 0.85,
      pagerank_iterations: 50,
      pagerank_convergence: 0.0001,

      min_relation_score: 0.4,
      max_connections_per_node: 20,
      bidirectional_relations: true
    }
  },

  // ---------------------------------------------------------------------------
  // TEXT PROCESSING: Optimized for Philosophical Dialogue
  // ---------------------------------------------------------------------------
  text_processing: {
    enabled: true,
    chunk_size: 800,                    // Large chunks for complete arguments
    chunk_overlap: 300,                 // High overlap for continuity
    min_chunk_size: 400,
    max_chunk_size: 1500,
    normalize_whitespace: true,
    remove_artifacts: true,

    enrichment: {
      enabled: true,
      auto_detect_format: true,
      parser_type: "auto",

      extract_keywords: true,
      max_keywords_per_chunk: 8,        // More keywords for philosophy
      use_tfidf: true,

      generate_summaries: true,         // Extractive summarization
      min_chunk_length_for_summary: 200,
      max_summary_length: 150,

      extract_chapter: true,
      extract_section: true,
      extract_position: true,
      calculate_confidence: true
    }
  },

  // ---------------------------------------------------------------------------
  // SUMMARIZATION: LLM-Based Hierarchical (Progressive)
  // ---------------------------------------------------------------------------
  // Riassunti gerarchici usando LLM per massima qualit√†
  // Approccio progressivo: estrattivo ‚Üí astrattivo per livelli pi√π alti
  summarization: {
    enabled: true,                          // Attiva riassunti gerarchici
    strategy: "progressive",                // Estrattivo poi astrattivo con LLM

    // Parametri per chunking gerarchico
    chunk_size: 1200,                       // Ottimizzato per dialoghi filosofici
    max_summary_length: 200,                // Riassunti concisi ma informativi
    levels: 3,                              // 3 livelli di astrazione
    overlap_sentences: 2,                   // Mantieni contesto tra chunks

    // LLM config per riassunti astrattivi
    llm_config: {
      enabled: true,                        // Usa LLM per riassunti di qualit√†
      model_name: "qwen3:8b-q4_k_m",        // Modello bilanciato qualit√†/velocit√†
      temperature: 0.3,                     // Bilanciamento creativit√†/accuratezza
      max_tokens: 180                       // Limita lunghezza riassunti
    },

    // Metodi estrattivi per livello base
    extractive_config: {
      use_tfidf: true,                      // TF-IDF per importanza frasi
      use_textrank: true,                   // TextRank per ranking
      sentence_window: 3,                   // Finestra per contesto
      min_sentence_length: 10,              // Min lunghezza frase
      max_sentences: 5                      // Max frasi nel riassunto estrattivo
    }
  },

  // ---------------------------------------------------------------------------
  // OLLAMA: Required for Full LLM Pipeline
  // ---------------------------------------------------------------------------
  ollama: {
    enabled: true,                      // MUST be enabled for semantic
    host: "http://localhost",
    port: 11434,
    chat_model: "qwen3:8b-q4_k_m",
    embedding_model: "nomic-embed-text",
    timeout_seconds: 90,
    max_retries: 3,
    fallback_to_hash: false,            // Don't fallback for semantic

    generation: {
      temperature: 0.2,                 // Balanced for philosophy
      top_p: 0.9,
      max_tokens: 1500,
      stream: false
    }
  },

  // ---------------------------------------------------------------------------
  // ENTITY EXTRACTION: Top-level gleaning configuration
  // ---------------------------------------------------------------------------
  entity_extraction: {
    enabled: true,
    min_confidence: 0.65,
    use_gleaning: true,
    max_gleaning_rounds: 4,
    semantic_merging: true,
    automatic_linking: true,
    linking_confidence_threshold: 0.7
  },

  // ---------------------------------------------------------------------------
  // QUERY PROCESSING: Advanced LLM Pipeline
  // ---------------------------------------------------------------------------
  query_processing: {
    enabled: true,
    use_advanced_pipeline: true,        // Full LLM pipeline
    use_intent_classification: true,
    use_concept_extraction: true,
    use_temporal_parsing: false,
    confidence_threshold: 0.5,

    intent_classification: {
      entity_patterns: ["who is", "speaker", "person", "character"],
      concept_patterns: ["what is", "define", "concept", "idea", "philosophy"],
      relationship_patterns: ["how does", "relationship", "connection", "according to"],
      search_patterns: ["find", "explain", "describe", "tell me about"]
    },

    prompt_templates: {
      entity: "Based on Plato's Symposium:\n\nQuery: {query}\n\nRelevant passages:\n{context}\n\nProvide a detailed answer:",
      concept: "Based on Plato's Symposium:\n\nQuery: {query}\n\nPhilosophical context:\n{context}\n\nExplain the concept:",
      relationship: "Based on Plato's Symposium:\n\nQuery: {query}\n\nRelevant arguments:\n{context}\n\nAnalyze the relationship:"
    }
  },

  // ---------------------------------------------------------------------------
  // PERFORMANCE: Balanced for Quality
  // ---------------------------------------------------------------------------
  performance: {
    batch_processing: true,
    batch_size: 16,                     // Smaller batches for LLM
    worker_threads: 4,
    memory_limit_mb: 4096,              // Higher for neural models
    cache_embeddings: true
  },

  // ---------------------------------------------------------------------------
  // MONITORING
  // ---------------------------------------------------------------------------
  monitoring: {
    enabled: true,
    track_entity_consistency: true,
    track_relationship_accuracy: true,
    log_insights: true
  },

  // ---------------------------------------------------------------------------
  // EXPERIMENTAL: Disabled for semantic
  // ---------------------------------------------------------------------------
  experimental: {
    neural_reranking: false,
    federated_learning: false,
    real_time_updates: false,
    distributed_processing: false,
    lazy_graphrag: false                // Don't use LazyGraphRAG for semantic
  }

}

// ==========================================================================
// SUMMARY: Full LLM Semantic Approach with Hierarchical Summarization
// ==========================================================================
// ‚úÖ LLM-based entity extraction with 4 gleaning rounds
// ‚úÖ Semantic embeddings (nomic-embed-text, 768-dim)
// ‚úÖ HNSW vector index for similarity search
// ‚úÖ PageRank graph scoring
// ‚úÖ Semantic merging of similar entities
// ‚úÖ Advanced query processing with LLM
// ‚úÖ Hierarchical summarization (progressive: extractive ‚Üí abstractive)
//
// üìä SUMMARIZATION FEATURES:
//   - Strategy: Progressive (extractive base + LLM abstractive)
//   - Levels: 3 (chunk ‚Üí section ‚Üí document)
//   - Methods: TF-IDF + TextRank (extractive) + LLM (abstractive)
//   - Quality: High-quality philosophical summaries
//
// üí∞ COSTS (Symposium 35k words):
//   - Indexing: ~$6-12 (LLM extraction + gleaning + embeddings + summarization)
//   - Query: ~$0.50 (embedding + LLM generation)
//   - Summarization adds: ~$1-2 to indexing cost
//
// ‚è±Ô∏è PERFORMANCE:
//   - Processing: 4-7 minutes (LLM-intensive + summarization)
//   - Retrieval: 2-3 seconds (embedding + vector search + LLM)
//   - Query total: ~2-3 seconds
//
// üéØ QUALITY: ~95% accuracy (maximum philosophical depth)
//   - Summaries: High quality, contextually rich
//   - Hierarchical structure improves query response
//
// ‚öñÔ∏è TRADE-OFFS:
//   - Higher cost vs algorithmic approach (+$1-2 for summaries)
//   - Longer processing time (4-7 min vs 10 sec)
//   - Best for: Deep analysis, academic research, complex queries
//   - Summaries improve answer quality and context understanding
// ==========================================================================
