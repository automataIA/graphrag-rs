# =============================================================================
# GraphRAG Configuration Template for NARRATIVE FICTION BOOKS
# Optimized for novels, literature, character-driven stories (100% DYNAMIC)
# ðŸŽ¯ TEXT-AGNOSTIC: Works with ANY book - Tom Sawyer, Symposium, or any literature
# =============================================================================

[general]
# General configuration
input_document_path = "info/The Adventures of Tom Sawyer.txt"
output_dir = "./output/tom_sawyer_optimized"
log_level = "info"
max_threads = 4
enable_profiling = true

[pipeline]
# Processing pipeline stages optimized for narrative
workflows = ["extract_text", "extract_entities", "build_graph", "detect_communities"]
parallel_execution = true

[pipeline.text_extraction]
# OPTIMIZED FOR NARRATIVE: Larger chunks to capture full scenes/conversations
chunk_size = 800              # â†‘ Increased from 512 - better for dialogue/scenes
chunk_overlap = 300           # â†‘ Increased from 200 - preserve character continuity
min_chunk_size = 200          # â†‘ Higher minimum for meaningful narrative units
clean_control_chars = true
normalize_whitespace = true

[pipeline.entity_extraction]
# NARRATIVE-SPECIFIC: Enhanced for literary characters and elements
model_name = "llama3.1:8b"
temperature = 0.1             # â†“ Lower for consistent character extraction
max_tokens = 1500             # â†‘ Higher for detailed character descriptions
entity_types = [
    "PERSON",                 # Main and secondary characters
    "CHARACTER_TRAIT",        # Personality, appearance, behavior
    "LOCATION",               # Settings, places
    "OBJECT",                 # Important items, symbols
    "EVENT",                  # Plot events, scenes
    "RELATIONSHIP",           # Character relationships
    "EMOTION",               # Emotional states, moods
    "THEME",                 # Literary themes, concepts
    "DIALOGUE_SPEAKER",      # Who said what
    "TIME_PERIOD"            # Temporal references
]
confidence_threshold = 0.6    # â†“ Slightly lower for literary nuance

[pipeline.entity_extraction.filters]
# NARRATIVE FILTERING: Optimized for character names and literary elements
min_entity_length = 2
max_entity_length = 100       # â†‘ Higher for full character names/descriptions
allowed_patterns = [
    "^[A-Z][a-zA-Z\\s'-]+$",  # Proper names
    "^[A-Z][a-z]+\\s+[A-Z][a-z]+$", # First Last names
    "^(Mr|Mrs|Miss|Dr|Uncle|Aunt)\\s+[A-Z][a-z]+$" # Titles + names
]
excluded_patterns = [
    "^(the|and|but|for|with|from)$", # Common words
    "^\\d+$",                         # Pure numbers
    "^[a-z]+$"                        # All lowercase (likely not names)
]

[pipeline.graph_building]
# NARRATIVE GRAPH: Focus on character relationships and plot connections
relation_scorer = "cosine_similarity"
min_relation_score = 0.4      # â†“ Lower for subtle literary relationships
max_connections_per_node = 25  # â†‘ Higher for complex character networks
bidirectional_relations = true
character_centrality_boost = 1.5  # Boost importance of main characters

[pipeline.community_detection]
# NARRATIVE COMMUNITIES: Character groups, subplots
algorithm = "leiden"
resolution = 0.6              # â†“ Lower for tighter character groupings
min_community_size = 2
max_community_size = 15       # â†“ Smaller for intimate character groups

[text_processing]
# Unified text processing settings
enabled = true
chunk_size = 800              # Consistent with pipeline setting
chunk_overlap = 300
min_chunk_size = 200
max_chunk_size = 1500         # â†‘ Allow longer chunks for complex scenes
normalize_whitespace = true
remove_artifacts = true
extract_keywords = true
keyword_min_score = 0.15      # â†‘ Higher threshold for quality

[entity_extraction]
# NARRATIVE ENTITY SETTINGS: Optimized for characters and literary elements
enabled = true
min_confidence = 0.6          # â†“ Lower for literary nuance
use_gleaning = true
max_gleaning_rounds = 4       # â†‘ More rounds for character details
gleaning_improvement_threshold = 0.08 # â†“ More sensitive improvement detection
semantic_merging = true
merge_similarity_threshold = 0.85     # â†‘ Higher precision for character merging
automatic_linking = true
linking_confidence_threshold = 0.7    # â†“ More aggressive character linking

[entity_extraction.gleaning]
# NARRATIVE GLEANING: Focus on character development and relationships
focus_areas = ["PERSON", "CHARACTER_TRAIT", "RELATIONSHIP", "EMOTION", "EVENT"]
context_window = 400          # â†‘ Larger context for character understanding
llm_temperature = 0.05        # â†“ Very low for consistent character analysis
narrative_context = true     # Enable narrative-specific processing
character_tracking = true    # Track character development over time

[graph_construction]
# NARRATIVE GRAPH: Character-centric network with plot relationships
enabled = true
incremental_updates = true
use_pagerank = true
pagerank_damping = 0.8        # â†“ Slightly lower for character importance
pagerank_iterations = 75     # â†‘ More iterations for complex narratives
pagerank_convergence = 0.00005 # â†‘ Higher precision
extract_relationships = true
relationship_confidence_threshold = 0.5 # â†“ Lower for subtle relationships
character_relationship_boost = 1.3     # Boost character-character connections

[vector_processing]
# SEMANTIC SEARCH: Optimized for narrative similarity
enabled = true
embedding_model = "nomic-embed-text"
embedding_dimensions = 768
use_hnsw_index = true
hnsw_ef_construction = 300    # â†‘ Higher for better narrative search
hnsw_m = 24                   # â†‘ More connections for semantic richness
similarity_threshold = 0.65   # â†“ Lower for narrative nuance
narrative_weighting = true   # Weight by narrative importance

[query_processing]
# NARRATIVE QUERIES: Enhanced for character and plot questions
enabled = true
use_advanced_pipeline = true
use_intent_classification = true
use_concept_extraction = true
use_temporal_parsing = true
confidence_threshold = 0.45   # â†“ Lower for literary interpretation
narrative_context_expansion = true # Expand queries with narrative context

[query_processing.intent_classification]
# NARRATIVE INTENT: Specialized patterns for literary analysis
character_patterns = ["who is", "character", "personality", "traits", "relationship"]
plot_patterns = ["what happens", "event", "scene", "chapter", "story"]
theme_patterns = ["theme", "meaning", "symbolism", "message", "represents"]
setting_patterns = ["where", "location", "place", "setting", "environment"]

[query_processing.prompt_templates]
# NARRATIVE PROMPTS: Specialized templates for literary analysis
character = """Based on the following context about characters and their relationships, provide a detailed character analysis for: {query}

Context:
{context}

Character Analysis:"""

plot = """Based on the following narrative context, describe the plot events and their significance for: {query}

Context:
{context}

Plot Description:"""

relationship = """Analyze the relationships and interactions described in the context for: {query}

Context:
{context}

Relationship Analysis:"""

[ollama]
# NARRATIVE LLM: Optimized for literary understanding
enabled = true
host = "http://localhost"
port = 11434
chat_model = "llama3.1:8b"
embedding_model = "nomic-embed-text"
timeout_seconds = 90          # â†‘ Higher for complex literary analysis
max_retries = 3
fallback_to_hash = false
max_tokens = 1200             # â†‘ Higher for detailed character analysis
temperature = 0.2             # â†“ Lower for consistent literary interpretation

[ollama.generation]
# NARRATIVE GENERATION: Optimized for literary responses
temperature = 0.3             # â†“ Lower for consistent literary analysis
top_p = 0.85                  # â†“ More focused responses
max_tokens = 1500             # â†‘ Longer for detailed character/plot analysis
stream = false
narrative_style = true       # Enable narrative-aware generation
character_voice_preservation = true # Maintain character voice consistency

[performance]
# NARRATIVE PERFORMANCE: Optimized for literary text processing
batch_processing = true
batch_size = 30               # â†“ Smaller batches for quality over speed
worker_threads = 4
memory_limit_mb = 6144        # â†‘ Higher for complex character networks
cache_embeddings = true       # Cache for repeated character mentions
character_cache_size = 1000   # Large cache for character consistency

[monitoring]
# NARRATIVE MONITORING: Track literary analysis quality
enabled = true
track_character_consistency = true
track_relationship_accuracy = true
track_plot_coherence = true
log_narrative_insights = true

# =============================================================================
# NARRATIVE-SPECIFIC OPTIMIZATIONS SUMMARY:
#
# 1. CHUNKING: Larger chunks (800) with more overlap (300) for scene continuity
# 2. ENTITIES: 10 specialized types including CHARACTER_TRAIT, EMOTION, THEME
# 3. CONFIDENCE: Lower thresholds (0.6) for literary nuance and subtlety
# 4. GRAPH: Character-centric with relationship boosting and tighter communities
# 5. LLM: Lower temperature (0.2) for consistent character analysis
# 6. CONTEXT: Larger windows (400) for character development tracking
# 7. SEARCH: Enhanced HNSW parameters for richer semantic connections
# =============================================================================